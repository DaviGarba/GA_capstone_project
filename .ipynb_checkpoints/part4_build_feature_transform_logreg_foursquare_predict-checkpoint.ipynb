{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 Build with Feature Extraction and Logistic Regression and Predict on Foursquare Data\n",
    "\n",
    "\n",
    "Part 4 Build + Predict\n",
    "BUILD: Create a data model\n",
    "- Selecting the appropriate model\n",
    "- Building a model\n",
    "- Testing and training our model\n",
    "- Evaluating and refining our model\n",
    "\n",
    "\n",
    "\n",
    "PREDICT: Predict on new data from foursquare API \n",
    "- loading the cleaned data from csv file\n",
    "- using the trained model create predictions \n",
    "- create visualisations to answer my question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training:\n",
    "- polarity : Target 2:positive, 1:neutral, 0:negative\n",
    "- words : preprocessed sentences\n",
    "- type : the tags of the words from lemmatizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training = pd.read_csv('./train_test_data/training_bs.csv', encoding='utf8')\n",
    "training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print training.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training['lem_words']\n",
    "y_train = training['sentiment']\n",
    "\n",
    "print X_train.shape\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Accuracy\n",
    "- The baseline accuracy is the proportion of the majority class. In this case '2' which is positive sentiment and so the baseline accuracy is 0.3\n",
    "\n",
    "baseline_accuracy = majority class N / total N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print y_train.value_counts(normalize=True)\n",
    "baseline = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Count Vec and TFIDF\n",
    "- Count Vectorizer \n",
    "- TFIDF Vectorizer \n",
    "- Logistic Regression is the classifier used.. very fast classifier???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr = LogisticRegression(random_state=1)\n",
    "\n",
    "##### Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# initalise the vectoriser \n",
    "cvec = CountVectorizer()\n",
    "# fit the training data on the model\n",
    "cvec.fit(X_train)\n",
    "\n",
    "#transform training data into sparse matrix\n",
    "X_train_cvec = cvec.transform(X_train)\n",
    "\n",
    "# cross val score/ predict\n",
    "cvec_score = cross_val_score(lr, X_train_cvec, y_train, cv=3)\n",
    "\n",
    "\n",
    "##### TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# initalise the vectoriser \n",
    "tvec = TfidfVectorizer()\n",
    "# fit the training data on the model\n",
    "tvec.fit(X_train)\n",
    "\n",
    "#transform training data into sparse matrix\n",
    "X_train_tvec = tvec.transform(X_train)\n",
    "\n",
    "# cross val score/ predict\n",
    "tvec_score = cross_val_score(lr, X_train_tvec, y_train, cv=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing scores\n",
    "- Both vectorizers increased the accuracy from the baseline\n",
    "- Count Vectorizer highest score\n",
    "- Hyperparameters will be further tested for the Count Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Baseline:', baseline\n",
    "print 'Tfidf Vectorizer Score:', tvec_score.mean()\n",
    "print 'Count Vectorizer Score:', cvec_score.mean()\n",
    "acc_list = []\n",
    "acc_list.append(cvec_score.mean())\n",
    "acc_list.append(tvec_score.mean())\n",
    "\n",
    "# DataFrame Accuracy \n",
    "acc_df = pd.DataFrame()\n",
    "acc_df['params']= ['cvec', 'tvec']\n",
    "acc_df['scores']= acc_list\n",
    "acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA of Count Vectorizer\n",
    "We still have the same number of rows but the vectorization has converted every word, or what is believed to be a word, from our test data into a feature. This is like dummy coded variables for words except that we have counts rather than just occurances.???.... featured names of wueds frequency of the top highest words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Frequency \n",
    "matrix outputting the 10 most common words and how many times they appear \n",
    "- Feature matrix of word occurences \n",
    "- top 10 word most occuring words\n",
    "- most important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cvec = pd.DataFrame(X_train_cvec.todense(),columns=cvec.get_feature_names())\n",
    "word_freq = df_cvec.sum(axis=0).sort_values(ascending=False)[:10]\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COUNT VEC - Zipf's law\n",
    "It state that \n",
    "In a corpus of text, any words frequency is inversely proportional to its rank in the frequency table. \n",
    "\n",
    "Thus the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc.: the rank-frequency distribution is an inverse relation\n",
    "\n",
    "\n",
    "Unsurprisingly, the word frequencies for the collected works of Alexandre Dumas appear to follow the predictions made by Zipf’s law! In this blog post, I’ll show you how to use bash and R to perform this simple analysis.\n",
    "\n",
    "\n",
    "Plotting the word frequency distribution, we observe : what is thissssssssss? https://en.wikipedia.org/wiki/Zipf%27s_law "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "word_freq.plot(kind='hist',\n",
    "            title='Number of words with a given number of appearances',\n",
    "            fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " in theory the more/less max features???? show compare write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Hyperparameters Count Vectorizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vec_ngram(params, X_train, y_train):\n",
    "    cvec_p = CountVectorizer(ngram_range=(params)) \n",
    "\n",
    "    cvec_p.fit(X_train)\n",
    "    X_train_cvec_p = cvec_p.transform(X_train)\n",
    "\n",
    "    # cross val score/ predict\n",
    "    cvec_score_p = cross_val_score(lr, X_train_cvec_p, y_train, cv=3)\n",
    "\n",
    "    # cross validation \n",
    "    return cvec_score_p.mean()\n",
    "\n",
    "params = [(1,1), (1,2),(1,3), (1,4)] \n",
    "ngram_scores = []\n",
    "for p in params:\n",
    "    ngram_scores.append(count_vec_ngram(p, X_train, y_train))\n",
    "    \n",
    "ngrams = ['cvec gram_1','cvec gram_2','cvec gram_3','cvec gram_4']\n",
    "ngram_df = pd.DataFrame({'params':ngrams, 'scores':ngram_scores}, index=[0,1,2,3])\n",
    "# adding cvec score with default params\n",
    "ngram_df = ngram_df.append(acc_df.iloc[:1,:])\n",
    "\n",
    "# plot scores on graph\n",
    "sns.pointplot(x='params', y='scores', data =ngram_df)\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.xlabel('ngrams')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update accuracy scores with highest score for 1,2 ngram\n",
    "acc_df1 = acc_df.append(ngram_df.iloc[3,:])\n",
    "acc_df1.reset_index(inplace=True, drop=True)\n",
    "acc_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vec_max_features(params, X_train, y_train):\n",
    "    cvec_p = CountVectorizer(max_features=params) \n",
    "\n",
    "    cvec_p.fit(X_train)\n",
    "    X_train_cvec_p = cvec_p.transform(X_train)\n",
    "\n",
    "    # cross val score/ predict\n",
    "    cvec_score_p = cross_val_score(lr, X_train_cvec_p, y_train, cv=3)\n",
    "\n",
    "    # cross validation \n",
    "    return cvec_score_p.mean()\n",
    "\n",
    "mf_params = [None, 500, 1000, 5000, 10000]\n",
    "max_features_scores = [count_vec_max_features(p, X_train, y_train) for p in mf_params]\n",
    "max_features = ['max_f_'+str(p) for p in mf_params]\n",
    "\n",
    "# dataframe for scores\n",
    "max_features_df = pd.DataFrame({'params':max_features, 'scores':max_features_scores}, index=[0,1,2,3,4])\n",
    "# adding cvec score with default params\n",
    "max_features_df = max_features_df.append(acc_df.iloc[:1,:])\n",
    "\n",
    "sns.pointplot(x='params', y='scores', data =max_features_df)\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.xlabel('ngrams')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update accuracy dataframe with 3 highest scores\n",
    "acc_df2 = acc_df1.append(max_features_df.drop(max_features_df.index[[1,2]]))\n",
    "acc_df2.reset_index(inplace=True, drop=True)\n",
    "acc_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vec_max_df(params, X_train, y_train):\n",
    "    cvec_p = CountVectorizer(max_df=params) \n",
    "\n",
    "    cvec_p.fit(X_train)\n",
    "    X_train_cvec_p = cvec_p.transform(X_train)\n",
    "\n",
    "    # cross val score/ predict\n",
    "    cvec_score_p = cross_val_score(lr, X_train_cvec_p, y_train, cv=3)\n",
    "\n",
    "    # cross validation \n",
    "    return cvec_score_p.mean()\n",
    "\n",
    "mdf_params = [0.25, 0.5, 0.75, 1.0]\n",
    "max_df_scores = [count_vec_max_df(p, X_train, y_train) for p in mdf_params]\n",
    "max_df = ['max_df_'+str(p) for p in mdf_params]\n",
    "\n",
    "# dataframe for scores\n",
    "max_df_df = pd.DataFrame({'params':max_df, 'scores':max_df_scores}, index=[0,1,2,3])\n",
    "# adding cvec score with default params\n",
    "max_df_df = max_df_df.append(acc_df.iloc[:1,:])\n",
    "\n",
    "sns.pointplot(x='params', y='scores', data =max_df_df)\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.xlabel('max_df')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update accuracy dataframe\n",
    "acc_df3 = acc_df2.append(max_df_df.iloc[:2,:])\n",
    "acc_df3.reset_index(inplace=True, drop=True)\n",
    "acc_df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Count Vectorizer with chosen parameters\n",
    "- ngram_range = (1,2): Bigram had the highest score\n",
    "- max_features: will not be used as the highest scores were the same as default params. \n",
    "- max_df =  0.25: Both 0.25 and 0.5 gave the same score. The general trend of this param is the lower the threshold the higher the score. so between the 0.5 and 0.25 the lowest is chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# initalise the vectoriser \n",
    "cvec = CountVectorizer(ngram_range=(1,4), max_df=0.25)\n",
    "# fit the training data on the model\n",
    "cvec.fit(X_train)\n",
    "\n",
    "#transform training data into sparse matrix\n",
    "X_train_cvec = cvec.transform(X_train)\n",
    "\n",
    "# cross val score/ predict\n",
    "cvec_score = cross_val_score(lr, X_train_cvec, y_train, cv=3)\n",
    "cvec_score.mean()\n",
    "\n",
    "\n",
    "acc_df3.loc[8]= ['best_params', cvec_score.mean()]\n",
    "acc_df3.sort_values('scores', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highest Score of Feature Transform Optimization: Bigram\n",
    "- As shown above the combined best parameters gave a lower score than bigram range with default params. \n",
    "- The Bigram range is then inputted as a fixed param to then optimize the Logistic Regression \n",
    "\n",
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# Transform features once!\n",
    "cvec_p = CountVectorizer(ngram_range=(1,4)) \n",
    "cvec_p.fit(X_train)\n",
    "X_train_cvec_p = cvec_p.transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_l1 = LogisticRegressionCV(Cs=np.logspace(-10,10,21),penalty = 'l1',solver='liblinear',cv=5) \n",
    "model_l1.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch on Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform features once!\n",
    "cvec_p = CountVectorizer(ngram_range=(1,4)) \n",
    "cvec_p.fit(X_train)\n",
    "X_train_cvec_p = cvec_p.transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = {'penalty': ['l1','l2'],\n",
    "          'solver':['liblinear'],\n",
    "          'C': np.logspace(-10,10,21)}\n",
    "\n",
    "# we define the gridsearchCV\n",
    "lr_grid = GridSearchCV(lr, param_grid=lr_params, cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "# fit with the tranformed sparse matrix\n",
    "lr_grid.fit(X_train_cvec_p, y_train)\n",
    "\n",
    "print 'Best Score:', lr_grid.best_score_\n",
    "print\n",
    "# assign the best estimator to a variable:\n",
    "best_lr = lr_grid.best_estimator_\n",
    "print 'Best Params:', lr_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression CV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "# from sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# try logistic regression CV???? instead of grid search???????\n",
    "\n",
    "statement...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "lr_cv = LogisticRegressionCV(Cs=np.logspace(-10,10,21),penalty = 'l1',solver='liblinear',cv=5) \n",
    "model_l1.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = pd.read_csv('./train_test_data/testing.csv')\n",
    "testing.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = testing.lem_words\n",
    "y_test = testing['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform with cvec and predict on best log reg\n",
    "X_test_mat = cvec_p.transform(X_test)\n",
    "y_pred = best_lr.predict(X_test_mat)\n",
    "\n",
    "print 'Best CVal on training:', lr_grid.best_score_\n",
    "print 'Best Model on testing:', accuracy_score(y_test, y_pred)\n",
    "print \"Number of classification errors:\", np.abs(y_pred - y_test).sum() \n",
    "print 'Total:', len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvec_p.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model created had had a high accuracy score of 0.94 \n",
    "- after scoring on test set it lowered to 0.67 \n",
    "- which is still significantly higher than the baseline of 0.3\n",
    "- however shows the model is overfitting on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Confusions matirix \n",
    "- I will evaluate my model through showing the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "confusion = pd.DataFrame(np.array(cm), index=['True_Negative', 'True_Neutral', 'True_Positive'], \n",
    "                         columns =['Pred_Negative', 'Pred_Neutral', 'Pred_Positive'])\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_class = ['Negative', 'Neutral', 'Positive']\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(cm, classes=x_class, title='Confusion matrix, without normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(cm, classes=x_class, normalize=True, title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "https://medium.com/tech-vision/introduction-to-confusion-matrix-classification-modeling-54d867169906\n",
    "\n",
    "https://www.youtube.com/watch?v=FAr2GmWNbT0 \n",
    "- TP TN FP FN??? \n",
    "\n",
    "- Accuracy is the ratio of True positive + True negative values over the whole population (ratio of correctly predicted values) \n",
    "\n",
    "- These models all have relatively high scores however their accuracy may not be the best due to their highly unbalanced classes. for instance if we were to predict a spam email with the ratio of 100 spam emails and 900 non-spam. Then we can only say that the prediction is only 10% accurate.\n",
    "\n",
    "- To increase the accruacy I would suggest resampling with replacement using boostrapping to balance the classes. For example spam emails could be upsampled to 500 and non spam down sampled to 500. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print classification_report(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Probabilities for Classes and Roc Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pp_mat =best_lr.predict_proba(X_test_mat)\n",
    "\n",
    "print 'predicted probabilities for each class:' \n",
    "# Get the predicted probability vector and explicitly name the columns:\n",
    "Y_pp = pd.DataFrame(Y_pp_mat, columns=['class_0_pp','class_1_pp', 'class_2_pp'])\n",
    "# Y_pp['pred_class_thresh10'] = [1 if x >= 0.10 else 0 for x in Y_pp.class_1_pp.values]\n",
    "Y_pp.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the output\n",
    "y_test_b = label_binarize(y_test, classes=[0, 1, 2])\n",
    "n_classes_test = y_test_b.shape[1]\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes_test):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_b[:, i], Y_pp_mat[:,i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "\n",
    "x_class = ['Negative', 'Neutral', 'Positive']\n",
    "# Plot of a ROC curve for a specific class\n",
    "plt.figure(figsize=[6,6])\n",
    "for i in range(n_classes_test):   \n",
    "        plt.plot(fpr[i], tpr[i], label= str(x_class[i])+ ' (area = %0.2f)' % roc_auc[i], linewidth=4)\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=18)\n",
    "plt.ylabel('True Positive Rate', fontsize=18)\n",
    "plt.title('ROC Curve for Sentiment Classification', fontsize=18)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_b.shape # check for variable in cell above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr = \n",
    "fpr = \n",
    "threshold = \n",
    "\n",
    "print 'fpr\\t', 'tpr\\t', 'threshold'\n",
    "print np.array(zip(fpr,tpr,threshold))\n",
    "# higher recall remins high even with threshold value\n",
    "# surface under curve area canoot be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvec_p.get_feature_names() checkkkkkkk \n",
    "# feature names which words effected the model most\n",
    "best_lr.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({\n",
    "        'coef':best_lr.coef_[0]})\n",
    "coef_df['abs_coef'] = np.abs(coef_df.coef)\n",
    "coef_df['feature_names'] = cvec_p.get_feature_names()\n",
    "# sort by absolute value of coefficient (magnitude)\n",
    "# coef_df.sort_values('abs_coef', ascending=False, inplace=True)\n",
    "\n",
    "coef_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show non-zero coefs and predictors\n",
    "# coef_df[coef_df.coef != 0]\n",
    "len(coef_df[coef_df.coef != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " is it similar??? \n",
    "# tvec_df  = pd.DataFrame(tvec.transform(training.words).todense(),\n",
    "#                    columns=tvec.get_feature_names())\n",
    "# tvec_df.sum(axis=0).sort_values(ascending=False)[:10]\n",
    "# top words...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foursquare data\n",
    "    - polarity : Target 2:positive, 1:neutral, 0:negative\n",
    "    - words : preprocessed sentences\n",
    "    - type : the tags of the words from lemmatizing \n",
    "\n",
    "\n",
    "TFDIF was fitted on the training data which will be used to transfomation the words in testing into a sparse matrix\n",
    "Logistic Regressions best parameters which were fit for the training data will then predict sentiment (y_hat) for the transformed testing data.\n",
    "\n",
    "\n",
    "\n",
    "---- logistic regression gave a good score in classifying the predictors but will try different classifiers to determine best accuracy score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foursquare = pd.read_csv('./foursquare_clean.csv', encoding='utf8')\n",
    "# foursquare = foursquare.dropna()\n",
    "X = foursquare.lem_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foursquare.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform the testing data\n",
    "X_mat = tvec.transform(X)\n",
    "\n",
    "# predictive probabilities\n",
    "\n",
    "# y_hat_pp = best_lr.predict_proba(X_mat)\n",
    "# y_hat_pp ???? can i ahve these on the foursquare data????\n",
    "\n",
    "# Y_pp = pd.DataFrame(best_lr.predict_proba(X_test_mat), columns=['class_0_pp','class_1_pp', 'class_2_pp'])\n",
    "# Y_pp.head(10) \n",
    "\n",
    "# probability in class 1 class 2 class 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foursquare['polarity_pred'] = best_lr.predict(X_mat)\n",
    "lng=[]\n",
    "lat=[]\n",
    "for ll in foursquare['ll']:\n",
    "    lnglat = ll.split(',')\n",
    "    lng.append(lnglat[0])\n",
    "    lat.append(lnglat[1])\n",
    "foursquare['lng'] =lng\n",
    "foursquare['lat'] =lat\n",
    "\n",
    "# dummies = pd.get_dummies(foursquare.polarity_pred)\n",
    "# dummies.columns = ['negative_0', 'neutral_1', 'positive_2']\n",
    "# foursquare_all = pd.concat([foursquare, dummies])\n",
    "\n",
    "foursquare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Save Foursquare Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foursquare.to_csv('foursquare_predictions.csv', header=True, index=False, encoding='UTF8')\n",
    "\n",
    "df_neutral= foursquare[foursquare.polarity_pred ==1]\n",
    "df_positive= foursquare[foursquare.polarity_pred ==2]\n",
    "df_negative= foursquare[foursquare.polarity_pred ==0]\n",
    "\n",
    "# created to make graphs on tableauWe choose parameters improve our analysis. There are some which can be applied but will not be used because the preprocessing step included them including stop words, lowercase and tokenizer. We will include parameters under a range which will be Grid Searched to give the optimal scores for the specific model in the pipeline. \n",
    "\n",
    "Parameters help show us how exactly we want to break down the document.\n",
    "\n",
    "- max_features : \n",
    "The maxiumum number of highest term frequencies across the corpus will only be considered as the vocabulary. \n",
    "- norm : (‘l1’, ‘l2’, None)????\n",
    "Norm used to normalize term vectors. None for no normalization.\n",
    "\n",
    "\n",
    "- ngram_range :(min_n, max_n) i should only use this if on raw text????\n",
    "The n-value range of lower and upper boundaries for n-grams to be extracted.\n",
    "- min_df \n",
    "specifies that a word must be used at least twice to be considered. In practice this is useful for removing things like URLs from text, which appear as one offs.\n",
    "\n",
    "- The max_df is a float value — this tell the vectorizer to ignore words which appears in more than 50% of documents in the corpus. This generally catches words not already defined in the stopwords set.\n",
    "\n",
    "- We also define a set of stop words. These are words like “a” or “is” which appear so often in a language that we know they won’t provide useful information and so can be ignored.\n",
    "\n",
    "\n",
    "\n",
    "http://apapiu.github.io/2016-08-04-tf_idf/\n",
    "\n",
    "- Finally we use fit_transform() to train the vectorizer using the corpus we defined above.\n",
    "\n",
    "\n",
    "sublinear_tf=True, max_features = 500 \n",
    "\n",
    "max_features : int or None, default=None\n",
    "    If not None, build a vocabulary that only consider the top\n",
    "    max_features ordered by term frequency across the corpus.\n",
    "\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "sublinear_tf : boolean, default=False\n",
    "    Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
    "TF-IDF: combination of sublinear TF and inverse document frequency???\n",
    "\n",
    "Term Frequency (TF)\n",
    "local frequency of a word in the document\n",
    "i.e. the word is weighed by how many times it occurs in the document\n",
    "tf(w,d)=∣∣{w′∈d : w′=w}∣∣tf(w,d)=|{w′∈d : w′=w}| where ww is a word and d={w1, ... ,wm}d={w1, ... ,wm} is a document\n",
    "\n",
    "Sublinear TF:\n",
    "sometimes a word is used too often so we want to reduce its influence compared to other less frequently used words\n",
    "for that we can use some sublinear function, e.g.\n",
    "logtf(w,d)log⁡tf(w,d) or tf(w,d)‾‾‾‾‾‾‾√\n",
    "\n",
    "http://0agr.ru/wiki/index.php/TF-IDF\n",
    "df_neutral.to_csv('fouresquare_predictions/df_neutral.csv', header=True, index=False, encoding='UTF8')\n",
    "df_positive.to_csv('fouresquare_predictions/df_positive.csv', header=True, index=False, encoding='UTF8')\n",
    "df_negative.to_csv('fouresquare_predictions/df_negative.csv', header=True, index=False, encoding='UTF8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats, integrate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foursquare.groupby(['polarity_pred']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(foursquare.polarity_pred, bins = 3, align= 'mid')\n",
    "plt.xticks(range(3), ['Negative','Neutral', 'Positive'])\n",
    "plt.xlabel('Predicted Sentiment of Reviews')\n",
    "plt.title('Distribution of Sentiment of Reviews')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES\n",
    "\n",
    "https://stackoverflow.com/questions/40679883/scikit-learn-how-to-include-others-features-after-performed-fit-and-transform-o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "later, additonal work, \n",
    "- clustering\n",
    "whats similar to logistic regression\n",
    "-neural networks\n",
    "from sklearn.neural_network import MLPClassifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We choose parameters improve our analysis. There are some which can be applied but will not be used because the preprocessing step included them including stop words, lowercase and tokenizer. We will include parameters under a range which will be Grid Searched to give the optimal scores for the specific model in the pipeline. \n",
    "\n",
    "Parameters help show us how exactly we want to break down the document.\n",
    "\n",
    "- max_features : \n",
    "The maxiumum number of highest term frequencies across the corpus will only be considered as the vocabulary. \n",
    "- norm : (‘l1’, ‘l2’, None)????\n",
    "Norm used to normalize term vectors. None for no normalization.\n",
    "\n",
    "\n",
    "- ngram_range :(min_n, max_n) i should only use this if on raw text????\n",
    "The n-value range of lower and upper boundaries for n-grams to be extracted.\n",
    "- min_df \n",
    "specifies that a word must be used at least twice to be considered. In practice this is useful for removing things like URLs from text, which appear as one offs.\n",
    "\n",
    "- The max_df is a float value — this tell the vectorizer to ignore words which appears in more than 50% of documents in the corpus. This generally catches words not already defined in the stopwords set.\n",
    "\n",
    "- We also define a set of stop words. These are words like “a” or “is” which appear so often in a language that we know they won’t provide useful information and so can be ignored.\n",
    "\n",
    "\n",
    "\n",
    "http://apapiu.github.io/2016-08-04-tf_idf/\n",
    "\n",
    "- Finally we use fit_transform() to train the vectorizer using the corpus we defined above.\n",
    "\n",
    "\n",
    "sublinear_tf=True, max_features = 500 \n",
    "\n",
    "max_features : int or None, default=None\n",
    "    If not None, build a vocabulary that only consider the top\n",
    "    max_features ordered by term frequency across the corpus.\n",
    "\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "sublinear_tf : boolean, default=False\n",
    "    Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
    "TF-IDF: combination of sublinear TF and inverse document frequency???\n",
    "\n",
    "Term Frequency (TF)\n",
    "local frequency of a word in the document\n",
    "i.e. the word is weighed by how many times it occurs in the document\n",
    "tf(w,d)=∣∣{w′∈d : w′=w}∣∣tf(w,d)=|{w′∈d : w′=w}| where ww is a word and d={w1, ... ,wm}d={w1, ... ,wm} is a document\n",
    "\n",
    "Sublinear TF:\n",
    "sometimes a word is used too often so we want to reduce its influence compared to other less frequently used words\n",
    "for that we can use some sublinear function, e.g.\n",
    "logtf(w,d)log⁡tf(w,d) or tf(w,d)‾‾‾‾‾‾‾√\n",
    "\n",
    "http://0agr.ru/wiki/index.php/TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "# iterate through folds 5-10\n",
    "for folds in range(5,11):\n",
    "    print '------------------------------------\\n'\n",
    "    print 'K:', folds\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    scores = cross_val_score(lr, X[predictors], y, cv=folds)\n",
    "    print \"Cross-validated scores:\", scores\n",
    "    print \"Mean CV R2:\", np.mean(scores)\n",
    "    print 'Std CV R2:', np.std(scores)\n",
    "    \n",
    "    # Make cross-validated predictions\n",
    "    predictions = cross_val_predict(model, X[predictors], y, cv=folds)\n",
    "    \n",
    "    r2 = metrics.r2_score(y, predictions)\n",
    "    print \"Cross-Predicted R2:\", r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
