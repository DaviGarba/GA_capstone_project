{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation notes\n",
    "http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "https://medium.com/tech-vision/introduction-to-confusion-matrix-classification-modeling-54d867169906\n",
    "\n",
    "https://www.youtube.com/watch?v=FAr2GmWNbT0 \n",
    "- TP TN FP FN??? \n",
    "\n",
    "- Accuracy is the ratio of True positive + True negative values over the whole population (ratio of correctly predicted values) \n",
    "\n",
    "- These models all have relatively high scores however their accuracy may not be the best due to their highly unbalanced classes. for instance if we were to predict a spam email with the ratio of 100 spam emails and 900 non-spam. Then we can only say that the prediction is only 10% accurate.\n",
    "\n",
    "- To increase the accruacy I would suggest resampling with replacement using boostrapping to balance the classes. For example spam emails could be upsampled to 500 and non spam down sampled to 500. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "l2 is ridgeeeeeeee\n",
    "- notes might help - Lasso was chosen: this indicates that maybe unimportant (noise) variables\n",
    "- is more of an issue in our data than multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params on testing data\n",
    "- TFDIF was fitted on the training data which will be used to transfomation the words in testing into a sparse matrix\n",
    "- This will then use the logistic regressions best parameters which were fit for thr training data to predict sentiment/ y_hat for my testing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "https://stackoverflow.com/questions/40679883/scikit-learn-how-to-include-others-features-after-performed-fit-and-transform-o\n",
    "    \n",
    "later, additonal work, \n",
    "- clustering\n",
    "whats similar to logistic regression\n",
    "-neural networks\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "\n",
    "We choose parameters improve our analysis. There are some which can be applied but will not be used because the preprocessing step included them including stop words, lowercase and tokenizer. We will include parameters under a range which will be Grid Searched to give the optimal scores for the specific model in the pipeline. \n",
    "\n",
    "Parameters help show us how exactly we want to break down the document.\n",
    "\n",
    "- max_features : \n",
    "The maxiumum number of highest term frequencies across the corpus will only be considered as the vocabulary. \n",
    "- norm : (‘l1’, ‘l2’, None)????\n",
    "Norm used to normalize term vectors. None for no normalization.\n",
    "\n",
    "\n",
    "- ngram_range :(min_n, max_n) i should only use this if on raw text????\n",
    "The n-value range of lower and upper boundaries for n-grams to be extracted.\n",
    "- min_df \n",
    "specifies that a word must be used at least twice to be considered. In practice this is useful for removing things like URLs from text, which appear as one offs.\n",
    "\n",
    "- The max_df is a float value — this tell the vectorizer to ignore words which appears in more than 50% of documents in the corpus. This generally catches words not already defined in the stopwords set.\n",
    "\n",
    "- We also define a set of stop words. These are words like “a” or “is” which appear so often in a language that we know they won’t provide useful information and so can be ignored.\n",
    "\n",
    "\n",
    "\n",
    "http://apapiu.github.io/2016-08-04-tf_idf/\n",
    "\n",
    "- Finally we use fit_transform() to train the vectorizer using the corpus we defined above.\n",
    "\n",
    "\n",
    "sublinear_tf=True, max_features = 500 \n",
    "\n",
    "max_features : int or None, default=None\n",
    "    If not None, build a vocabulary that only consider the top\n",
    "    max_features ordered by term frequency across the corpus.\n",
    "\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "sublinear_tf : boolean, default=False\n",
    "    Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
    "TF-IDF: combination of sublinear TF and inverse document frequency???\n",
    "\n",
    "Term Frequency (TF)\n",
    "local frequency of a word in the document\n",
    "i.e. the word is weighed by how many times it occurs in the document\n",
    "tf(w,d)=∣∣{w′∈d : w′=w}∣∣tf(w,d)=|{w′∈d : w′=w}| where ww is a word and d={w1, ... ,wm}d={w1, ... ,wm} is a document\n",
    "\n",
    "Sublinear TF:\n",
    "sometimes a word is used too often so we want to reduce its influence compared to other less frequently used words\n",
    "for that we can use some sublinear function, e.g.\n",
    "logtf(w,d)log⁡tf(w,d) or tf(w,d)‾‾‾‾‾‾‾√\n",
    "\n",
    "http://0agr.ru/wiki/index.php/TF-IDFtesting.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform the testing data\n",
    "X_test_testing = tvec.transform(testing.words)\n",
    "\n",
    "# predictions and predictive probabilities\n",
    "y_hat = best_lr_tvec.predict(X_test_testing)\n",
    "y_hat_pp = best_lr_tvec.predict_proba(X_test_testing)\n",
    "y_hat_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic regression gave a good score in classifying the predictors but will try different classifiers to determine best accuracy score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_lr_tvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Params on testing data\n",
    "- TFDIF was fitted on the training data which will be used to transfomation the words in testing into a sparse matrix\n",
    "- This will then use the logistic regressions best parameters which were fit for thr training data to predict sentiment/ y_hat for my testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tvec_df  = pd.DataFrame(tvec.transform(training.words).todense(),\n",
    "#                    columns=tvec.get_feature_names())\n",
    "# tvec_df.sum(axis=0).sort_values(ascending=False)[:10]\n",
    "# top words...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES\n",
    "\n",
    "https://stackoverflow.com/questions/40679883/scikit-learn-how-to-include-others-features-after-performed-fit-and-transform-o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "later, additonal work, \n",
    "- clustering\n",
    "whats similar to logistic regression\n",
    "-neural networks\n",
    "from sklearn.neural_network import MLPClassifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
