{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Evaluation\n",
    "- feature importances - from the end of grid search lesson\n",
    "\n",
    "best_lr.coef_[0]\n",
    "In [60]:\n",
    "coef_df = pd.DataFrame({\n",
    "        'coef':best_lr.coef_[0],\n",
    "        'feature':X.columns })\n",
    "In [69]:\n",
    "coef_df['abs_coef'] = np.abs(coef_df.coef)\n",
    "# sort by absolute value of coefficient (magnitude)\n",
    "coef_df.sort_values('abs_coef', ascending=False, inplace=True)\n",
    "In [70]:\n",
    "# Show non-zero coefs and predictors\n",
    "coef_df[coef_df.coef != 0]\n",
    "Out[70]:\n",
    "coef\tfeature\tabs_coef\n",
    "8\t0.218062\tgame_behind\t0.218062\n",
    "In [38]:\n",
    "# Game_behind, home team offensive rating average over the last ten games,\n",
    "# and to a lesser extend the defensive rating are predictors of the home\n",
    "# team winning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grammar checking: \n",
    "Grammar checking is majorly learning based, huge amount of proper text data is learned and models are created for the purpose of grammar correction. There are many online tools that are available for grammar correction purposes.\n",
    "#### Spelling correction: \n",
    "In natural language, misspelled errors are encountered. Companies like Google and Microsoft have achieved a decent accuracy level in automated spell correction. One can use algorithms like the Levenshtein Distances, Dictionary Lookup etc. or other modules and packages to fix these errors.\n",
    "\n",
    "- negation handling \n",
    "Negative words are generally just words that you don't want to analyze because they carry no meaning. They may also be likely sarcastic words (usually very bombastic words). You handle them by simply removing them entirely, or not considering them for applying sentiment to. https://www.quora.com/How-do-I-include-negations-of-words-in-sentiment-analysis https://stackoverflow.com/questions/29374157/negation-handling-in-sentiment-analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Standardization\n",
    "- mispellings and slang\n",
    "text which are not in proper formats so are not in standard lexical dictionaries therefore cannot be recognised. slag words should be transformed into standard words to make free text. Reviews can contain... dictionary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lookup_dict = {'rt':'Retweet', 'dm':'direct message', \"awsm\" : \"awesome\", \"luv\" :\"love\", \"...\"}\n",
    "# def _lookup_words(input_text):\n",
    "#     words = input_text.split() \n",
    "#     new_words = [] \n",
    "#     for word in words:\n",
    "#         if word.lower() in lookup_dict:\n",
    "#             word = lookup_dict[word.lower()]\n",
    "#         new_words.append(word) new_text = \" \".join(new_words) \n",
    "#         return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Chosen estimatro \n",
    "best_lr_tvec.predic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# only on the training data.... to test the model\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "confusion_matrix(y_test_tvec, y_hat)\n",
    "# y_train_tvec\n",
    "conmat = np.array(confusion_matrix(y_test, yhat, labels=[1,0]))\n",
    "\n",
    "# confusion = pd.DataFrame(conmat, index=['is_male', 'is_female'],\n",
    "#                          columns=['predicted_male','predicted_female'])\n",
    "confusionPrediction MetricsPrediction Metrics\n",
    "\n",
    "print accuracy_score(y_train_tvec, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-b4fc85ba2b1e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-b4fc85ba2b1e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    describe confusion matrix? do i need it yet or after gridsearch on more models???\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "describe confusion matrix? do i need it yet or after gridsearch on more models???\n",
    "$$\n",
    "\\mathrm{ACC} = \\left(\\frac{Σ \\text{ True positive} + Σ \\text{ True negative}}{Σ\\text{ Total population}}\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
