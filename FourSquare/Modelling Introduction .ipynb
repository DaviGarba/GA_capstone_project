{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df['rating']\n",
    "predictors = ['tips'(need sentiment), 'cat_name'(dummify), 'll'(separate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PCA\n",
    "maybe clusting with the foursquare data\n",
    "- DBSCAN?\n",
    "https://git.generalassemb.ly/DSI-LDN-4/lessons-repo/blob/master/week07/day3_hierachical_clustering/clustering-dbscan-lab/practice-dbscan-solutions.ipynb \n",
    "https://git.generalassemb.ly/DSI-LDN-4/lessons-repo/blob/master/week07/day3_hierachical_clustering/clustering-dbscan-lesson/DBSCAN.pdf \n",
    "looking at clustering densities then printing into a map of london..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accuracy_score(y_train, estimator.predict(X_test))\n",
    "creating a class estimator\n",
    "\n",
    "class estimator_selection:\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "    \n",
    "    def fit(self, X, y, cv=3, n_jobs=1, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs, \n",
    "                              verbose=verbose, scoring=scoring, refit=refit)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs    \n",
    "    \n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator': key,\n",
    "                 'min_score': min(scores),\n",
    "                 'max_score': max(scores),\n",
    "                 'mean_score': mean(scores),\n",
    "                 'std_score': std(scores),\n",
    "            }\n",
    "            return pd.Series(dict(params.items() + d.items()))\n",
    "                      \n",
    "\n",
    "\n",
    "params = { 'LogisticRegression': ...,\n",
    "    \n",
    "            'RandomForestClassifier': : { 'n_estimators': [16, 32] },\n",
    "                                {'max_depth':[1,2,3,4,5,6],\n",
    "                          #, 'max_features':[1,2,3,4], \n",
    "                          'max_leaf_nodes':[5,6,7,8,9,10], \n",
    "                          'min_samples_leaf':[1,2,3,4]\n",
    "                          #'min_samples_split':[1,2,3,4]\n",
    "                         }\n",
    "    \n",
    "#     'KNeighborsClassifier':{'n_neighbors':[1,3,5,9,15,21], 'weights':['uniform','distance'],\n",
    "#                             'metric':['euclidean','manhattan'],parameter_grid = {\n",
    "#                     'n_neighbors':range(1,151), 'weights':['uniform','distance',custom_function],\n",
    "#                     'algorithm':['ball_tree','kd_tree','brute','auto'],'leaf_size':range(1,152),\n",
    "#                         'metric':['minkowski','euclidean'], 'p':[1,2]\n",
    "    \n",
    "#     'SVC': [{'kernel': ['linear'], 'C': [1, 10]},\n",
    "#             {'kernel': ['rbf'], 'C': [1, 10], 'gamma': [0.001, 0.0001]}\n",
    "\n",
    "#     'MultinomialNB': MultinomialNB(),\n",
    "    \n",
    "#     'SGDClassifier': { 'alpha': (0.00001, 0.000001),\n",
    "#                       'penalty': ('l2', 'elasticnet'),\n",
    "#                       'n_iter': (10, 50, 80)}\n",
    "    \n",
    "# }\n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud once classified for positive and negative \n",
    "- get mask for tick and cross positive and negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://minimaxir.com/2016/05/wordclouds/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib as plt\n",
    "wordcloud = WordCloud(max_font_size=40).generate(df['tips'])\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://www.kaggle.com/futurist/positive-vs-negative-word-clouds/code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models\n",
    "Logistic Regression, Random Forest, KNN, SVC, Naive Bayes, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = {'LogisticRegression': LogisticRegression(),\n",
    "        'RandomForestClassifier': RandomForestClassifier()\n",
    "## Pipline with Chosen Classifier???\n",
    "    \n",
    "\n",
    "\n",
    "look at previous...\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', OneVsRestClassifier(MultinomialNB(\n",
    "        fit_prior=True, class_prior=None))),\n",
    "])\n",
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'clf__estimator__alpha': (1e-2, 1e-3)\n",
    "}\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=2, verbose=3)\n",
    "grid_search_tune.fit(train_x, train_y)\n",
    "\n",
    "print(\"Best parameters set:\")\n",
    "print grid_search_tune.best_estimator_.steps\n",
    "\n",
    "#### Logistic Regression\n",
    "\n",
    "params = {'penalty': ['l1','l2'],\n",
    "          'solver':['liblinear'],\n",
    "          'C': np.logspace(-10,10,21)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Decision Trees\n",
    "\n",
    "}\n",
    "#     'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "#     'SVC': SVC(),\n",
    "#     'MultinomialNB': MultinomialNB(),\n",
    "#     'SGDClassifier': SGDClassifier()\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = { 'LogisticRegression': ...,\n",
    "    \n",
    "            'RandomForestClassifier': : { 'n_estimators': [16, 32] },\n",
    "                                {'max_depth':[1,2,3,4,5,6],\n",
    "                          #, 'max_features':[1,2,3,4], \n",
    "                          'max_leaf_nodes':[5,6,7,8,9,10], \n",
    "                          'min_samples_leaf':[1,2,3,4]\n",
    "                          #'min_samples_split':[1,2,3,4]\n",
    "                         }\n",
    "    \n",
    "#     'KNeighborsClassifier':{'n_neighbors':[1,3,5,9,15,21], 'weights':['uniform','distance'],\n",
    "#                             'metric':['euclidean','manhattan'],parameter_grid = {\n",
    "#                     'n_neighbors':range(1,151), 'weights':['uniform','distance',custom_function],\n",
    "#                     'algorithm':['ball_tree','kd_tree','brute','auto'],'leaf_size':range(1,152),\n",
    "#                         'metric':['minkowski','euclidean'], 'p':[1,2]\n",
    "    \n",
    "#     'SVC': [{'kernel': ['linear'], 'C': [1, 10]},\n",
    "#             {'kernel': ['rbf'], 'C': [1, 10], 'gamma': [0.001, 0.0001]}\n",
    "\n",
    "#     'MultinomialNB': MultinomialNB(),\n",
    "    \n",
    "#     'SGDClassifier': { 'alpha': (0.00001, 0.000001),\n",
    "#                       'penalty': ('l2', 'elasticnet'),\n",
    "#                       'n_iter': (10, 50, 80)}\n",
    "    \n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating a class estimator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class estimator_selection:\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "    \n",
    "    def fit(self, X, y, cv=3, n_jobs=1, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs, \n",
    "                              verbose=verbose, scoring=scoring, refit=refit)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs    \n",
    "    \n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator': key,\n",
    "                 'min_score': min(scores),\n",
    "                 'max_score': max(scores),\n",
    "                 'mean_score': mean(scores),\n",
    "                 'std_score': std(scores),\n",
    "            }\n",
    "            return pd.Series(dict(params.items() + d.items()))\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(StandardScaler(), Lasso())\n",
    "pprint(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define parameters for the gridsearch\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    #'clf__n_iter': (10, 50, 80),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pipline with Chosen Classifier???\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = {'LogisticRegression': LogisticRegression(),\n",
    "        'RandomForestClassifier': RandomForestClassifier()\n",
    "## Pipline with Chosen Classifier???\n",
    "    \n",
    "\n",
    "\n",
    "look at previous...\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', OneVsRestClassifier(MultinomialNB(\n",
    "        fit_prior=True, class_prior=None))),\n",
    "])\n",
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'clf__estimator__alpha': (1e-2, 1e-3)\n",
    "}\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=2, verbose=3)\n",
    "grid_search_tune.fit(train_x, train_y)\n",
    "\n",
    "print(\"Best parameters set:\")\n",
    "print grid_search_tune.best_estimator_.steps\n",
    "\n",
    "#### Logistic Regression\n",
    "\n",
    "params = {'penalty': ['l1','l2'],\n",
    "          'solver':['liblinear'],\n",
    "          'C': np.logspace(-10,10,21)}\n",
    "\n",
    "\n",
    "\n",
    "#### Decision Trees\n",
    "\n",
    "}\n",
    "#     'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "#     'SVC': SVC(),\n",
    "#     'MultinomialNB': MultinomialNB(),\n",
    "#     'SGDClassifier': SGDClassifier()\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = {'LogisticRegression': LogisticRegression(),\n",
    "        'RandomForestClassifier': RandomForestClassifier()\n",
    "## Pipline with Chosen Classifier???\n",
    "    \n",
    "\n",
    "\n",
    "look at previous...\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', OneVsRestClassifier(MultinomialNB(\n",
    "        fit_prior=True, class_prior=None))),\n",
    "])\n",
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'clf__estimator__alpha': (1e-2, 1e-3)\n",
    "}\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=2, verbose=3)\n",
    "grid_search_tune.fit(train_x, train_y)\n",
    "\n",
    "print(\"Best parameters set:\")\n",
    "print grid_search_tune.best_estimator_.steps\n",
    "\n",
    "#### Logistic Regression\n",
    "\n",
    "params = {'penalty': ['l1','l2'],\n",
    "          'solver':['liblinear'],\n",
    "          'C': np.logspace(-10,10,21)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Decision Trees\n",
    "\n",
    "}\n",
    "#     'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "#     'SVC': SVC(),\n",
    "#     'MultinomialNB': MultinomialNB(),\n",
    "#     'SGDClassifier': SGDClassifier()\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARAMS\n",
    "params = { 'LogisticRegression': ...,\n",
    "    \n",
    "            'RandomForestClassifier': : { 'n_estimators': [16, 32] },\n",
    "                                {'max_depth':[1,2,3,4,5,6],\n",
    "                          #, 'max_features':[1,2,3,4], \n",
    "                          'max_leaf_nodes':[5,6,7,8,9,10], \n",
    "                          'min_samples_leaf':[1,2,3,4]\n",
    "                          #'min_samples_split':[1,2,3,4]\n",
    "                         }\n",
    "    \n",
    "#     'KNeighborsClassifier':{'n_neighbors':[1,3,5,9,15,21], 'weights':['uniform','distance'],\n",
    "#                             'metric':['euclidean','manhattan'],parameter_grid = {\n",
    "#                     'n_neighbors':range(1,151), 'weights':['uniform','distance',custom_function],\n",
    "#                     'algorithm':['ball_tree','kd_tree','brute','auto'],'leaf_size':range(1,152),\n",
    "#                         'metric':['minkowski','euclidean'], 'p':[1,2]\n",
    "    \n",
    "#     'SVC': [{'kernel': ['linear'], 'C': [1, 10]},\n",
    "#             {'kernel': ['rbf'], 'C': [1, 10], 'gamma': [0.001, 0.0001]}\n",
    "\n",
    "#     'MultinomialNB': MultinomialNB(),\n",
    "    \n",
    "#     'SGDClassifier': { 'alpha': (0.00001, 0.000001),\n",
    "#                       'penalty': ('l2', 'elasticnet'),\n",
    "#                       'n_iter': (10, 50, 80)}\n",
    "    \n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = {'LogisticRegression': LogisticRegression(),\n",
    "        'RandomForestClassifier': RandomForestClassifier()\n",
    "## Pipline with Chosen Classifier???\n",
    "    \n",
    "\n",
    "\n",
    "look at previous...\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', OneVsRestClassifier(MultinomialNB(\n",
    "        fit_prior=True, class_prior=None))),\n",
    "])\n",
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'clf__estimator__alpha': (1e-2, 1e-3)\n",
    "}\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=2, verbose=3)\n",
    "grid_search_tune.fit(train_x, train_y)\n",
    "\n",
    "print(\"Best parameters set:\")\n",
    "print grid_search_tune.best_estimator_.steps\n",
    "\n",
    "#### Logistic Regression\n",
    "\n",
    "params = {'penalty': ['l1','l2'],\n",
    "          'solver':['liblinear'],\n",
    "          'C': np.logspace(-10,10,21)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Decision Trees\n",
    "\n",
    "}\n",
    "#     'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "#     'SVC': SVC(),\n",
    "#     'MultinomialNB': MultinomialNB(),\n",
    "#     'SGDClassifier': SGDClassifier()\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating a class estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class estimator_selection:\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "    \n",
    "    def fit(self, X, y, cv=3, n_jobs=1, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs, \n",
    "                              verbose=verbose, scoring=scoring, refit=refit)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs    \n",
    "    \n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator': key,\n",
    "                 'min_score': min(scores),\n",
    "                 'max_score': max(scores),\n",
    "                 'mean_score': mean(scores),\n",
    "                 'std_score': std(scores),\n",
    "            }\n",
    "            return pd.Series(dict(params.items() + d.items()))\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "look at previous...\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', OneVsRestClassifier(MultinomialNB(\n",
    "        fit_prior=True, class_prior=None))),\n",
    "])\n",
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'clf__estimator__alpha': (1e-2, 1e-3)\n",
    "}\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=2, verbose=3)\n",
    "grid_search_tune.fit(train_x, train_y)\n",
    "\n",
    "print(\"Best parameters set:\")\n",
    "print grid_search_tune.best_estimator_.steps\n",
    "\n",
    "#### Logistic Regression\n",
    "\n",
    "params = {'penalty': ['l1','l2'],\n",
    "          'solver':['liblinear'],\n",
    "          'C': np.logspace(-10,10,21)}\n",
    "\n",
    "\n",
    "#### Decision Trees\n",
    "\n",
    "}\n",
    "#     'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "#     'SVC': SVC(),\n",
    "#     'MultinomialNB': MultinomialNB(),\n",
    "#     'SGDClassifier': SGDClassifier()\n",
    "# }\n",
    "\n",
    "\n",
    "params = { 'LogisticRegression': ...,\n",
    "    \n",
    "            'RandomForestClassifier': : { 'n_estimators': [16, 32] },\n",
    "                                {'max_depth':[1,2,3,4,5,6],\n",
    "                          #, 'max_features':[1,2,3,4], \n",
    "                          'max_leaf_nodes':[5,6,7,8,9,10], \n",
    "                          'min_samples_leaf':[1,2,3,4]\n",
    "                          #'min_samples_split':[1,2,3,4]\n",
    "                         }\n",
    "    \n",
    "#     'KNeighborsClassifier':{'n_neighbors':[1,3,5,9,15,21], 'weights':['uniform','distance'],\n",
    "#                             'metric':['euclidean','manhattan'],parameter_grid = {\n",
    "#                     'n_neighbors':range(1,151), 'weights':['uniform','distance',custom_function],\n",
    "#                     'algorithm':['ball_tree','kd_tree','brute','auto'],'leaf_size':range(1,152),\n",
    "#                         'metric':['minkowski','euclidean'], 'p':[1,2]\n",
    "    \n",
    "#     'SVC': [{'kernel': ['linear'], 'C': [1, 10]},\n",
    "#             {'kernel': ['rbf'], 'C': [1, 10], 'gamma': [0.001, 0.0001]}\n",
    "\n",
    "#     'MultinomialNB': MultinomialNB(),\n",
    "    \n",
    "#     'SGDClassifier': { 'alpha': (0.00001, 0.000001),\n",
    "#                       'penalty': ('l2', 'elasticnet'),\n",
    "#                       'n_iter': (10, 50, 80)}\n",
    "    \n",
    "# }\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
